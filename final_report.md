# Introduction

With this project, we want to try and fine-tune a model that is focused on generating mermaidJS diagrams for code related tasks.

The main goals are to match or even beat gpt-4-turbo in terms of Accuracy (defined by how many generation have valid syntax), Quality (which is a subjective measurement of the quality of the diagrams) and Speed

# Plan

We'll break this challenge in 3 main step - "Generate Dataset", "Fine-tune model" and "Evaluate"

- Generate dataset for training
  - Use dataset that was provided and filter questions that are programming related (using GPT)
  - Ask GPT to choose the appropriate diagram type for the each answer
  - Ask GPT to generate the diagram based on the diagram type and its documentation
  - Evaluate output for valid syntax, if incorrect, repeat
  - Prepare jsonl docs for training
  - (for later) Further refine the answer to improve quality and syntax
    - feed the validation error back to gpt to fix
    - provide better documentation or more examples of valid syntax
    - instruct the model to simplify the syntax
    - Ask GPT to evaluate the diagram
    - Ask GPT to generate a description of a diagram and have a model just creating the syntax❗
- Fine-tuning an openAI model
  - Include in prompt:
    - Diagram type
    - Question and answer context
    - Diagram type documentation (should we include the docs or not? - probably not) (ok to have the docs if in inference we don't need to send the docs)
  - Upload training and testing dataset to openAI
  - Run job 
- Evaluate performance of fine-tuned model:
  - Check for syntax validity (how many failed attempts with GPT-4 vs with fine-tuned model?)
  - Test performance of models with and without the documentation in the prompt
  - Ask GPT-4 to evaluate a subset
  - Choose random scenarios and make a subjective comparison between results of both models

# Generate Dataset

For the dataset generation we used a JSON dataset that includes user-data collected from Adrenaline. This dataset has ~2000 examples of diagrams that were generated, along with the user question and answer that was provided.

We used some techniques to improve the generation of the diagrams generated by gpt-4-turbo (the model we used to generate the dataset):
- We filtered questions that are related to software development
- We asked GPT to decide on what type of diagram to generate as mermaidJS has multiple and we restricted to list to the most useful ones
- We provided GPT with the documentation for the select mermaidJS diagram type when asked to generate, along with an explanation on why the type was selected, the user question and answer 
- We then validated the diagram generated using mermaidJS node module and if the syntax was not valid we asked gpt to try again with a max of 5 tries. 
  - We later found that the mermaidJS API available is not working properly. In the "Evaluate" stage we used their CLI

Because the documentation for each diagram type is extensive and the retries necessary, this process is costly:

For a full 200 sample (last 200)					
					
Prompt	1636893	$0.01	$16.36893		Cost was $18 for 200 samples
Completion	69409	$0.03	$2.08227		But this only resulted in 42 samples in the final dataset

❗We end-up generating a dataset for training on only 200 of the ~2000 user questions provided. And because many failed to generate a diagram with proper syntax (according to the buggy mermaidJS API) we only got 42 samples for our fine-tuning. Increasing the dataset will likely improve the results of the trained model❗

The data was split in a 80-20 ratio for training and testing.

# Fine-tune model

For fine tuning we decided to use OpenAI's gpt-3.5-turbo. We choose OpenAI's (instead of an OSS model) because it is easy and fast to do, which is important for the short timeline set for this project. It is also cheap to train and easy to use for inference later.

The model was training for 100 steps with the resulting metrics:
Training loss: 0.04987594857811928, 
Mean token accuracy: 0.9814814925193787

Name of trained mode: "ft:gpt-3.5-turbo-0613:rubrick-ai::8wWjQ3wc"

# Evaluate

To evaluate the performance of the model we compared it with gpt-4-turbo using a random subset of 100 samples from the original dataset.

We prompted both models with the same instruction that include the user's question, the answer provided and some instructions describing the task. We didn't include the documentation of mermaidJS that we used to generate the dataset.

In this 100 sample test both models had the same accuracy - 71%

´´´
Evaluation completed
First model (gpt-4-0125-preview) success rate: 71.0%
Second model (ft:gpt-3.5-turbo-0613:rubrick-ai::8wWjQ3wc) success rate: 71.0%
´´´

This means that the fine-tuned model still fails often to produce a correct diagram
On the other hand, we were able to have gpt-3.5-turbo matching the accuracy of gpt-4-turbo. This is specially remarkable because gpt-3.5-turbo (fine-tuned) is substantially cheaper to run:
```
gpt-4-0125-preview	$0.01 / 1K tokens	$0.03 / 1K tokens
gpt-3.5-turbo	$0.0030 / 1K tokens	$0.0060 / 1K tokens
```

It also much faster, with inference times that are sometimes 10x lower.

## Checking some examples manually

### Failed diagrams
This generation failed because of "Microcontroller(s)". Mermaid doesn't allow the use of brackets like that. 
```mermaid
graph LR
A[Trash Identification] --> B{AI and Machine Learning Algorithms}
B --> C[Identify waste type]
B --> D[Categorize waste into: <br> - Landfill <br> - Recyclables <br> - Combustibles]
A --> E{Computer Vision}
E --> F[Real-time image processing]
E --> G[Recognize trash types]
A --> H[Sorting]
H --> I[Internal Robotics]
I --> J[Sort waste based on category]
I --> K[Direct waste to corresponding internal bins]
H --> L[Microcontroller(s)]
L --> M[Control robotic components]
L --> N[Ensure waste is directed to the appropriate bin]
A --> O[User Interaction]
O --> P[Waste Depositing]
O --> Q[Waste Management]
Q --> R[Separate bins for each trash category]
R --> S[Landfill bin]
R --> T[Recyclables bin]
R --> U[Combustibles bin]
```

Again, this generation failed because of createPullRequest()
```
graph LR
A[UpdateGardenRepositoryModal.ts] --> B[Disable PR creation]
A[UpdateGardenRepositoryModal.ts] --> C[Check connection status]
A[UpdateGardenRepositoryModal.ts] --> D[Initialize GitHub settings]
B --> E[Modify Publisher.ts]
E --> F[Disable createPullRequest()]
E --> G[Add code to save HTML locally]
C --> H[Modify GithubSettings.ts]
H --> I[Remove or modify initialization code]
```

There seems to be a pattern here. We we delete "(a, b)" then the diagram becomes valid
```
graph LR
A[User Input] -->|a, b, f| B[Check Operation]
B -->|f == +| C[module.sum(a, b)]
B -->|f == -| D[module.sub(a, b)]
B -->|f == *| E[module.mul(a, b)]
B -->|f == /| F[module.div(a, b)]
C --> G[Print Result]
D --> G
E --> G
F --> G
G -->|Error| H[Print '\u041e\u0448\u0438\u0431\u043a\u0430']
```

In this example the diagram fails because the last line has extra "[]". Deleting them solves the problem
```
graph LR
A[isHeaderPanelVisible] -->|true| B[applayout__header]
A -->|false| C[isSubHeaderPanelVisible]
C -->|true| D[applayout__sub__header]
C -->|false| E[isLeftPanelVisible]
E -->|true| F[applayout__leftpanel]
E -->|false| G[applayout__mainpanel]
B -->|ng-content| H[skan-ui-header]
D -->|ng-content| I[skan-sub-header]
F -->|ng-scrollbar| J[ng-scrollbar]
J -->|ng-content| K[skan-ui-left-panel]
G -->|ng-content| L[skan-ui-main-panel, [main-content]]
```

**The model seems to understand how to properly generate the diagrams, but fails by included special/reserved characters where it shouldn't add them. We should investigate if the training data includes enough examples to show the model how to properly handles this cases.**

### Comparing quality between models

Comparing quality of the output is subjective. We'll compare some example of outputs of our model with the output that gpt-4-turbo generated for the same prompt and assess if we can see substantial differences in quality.

https://mermaid.live/ can be used to visualize the diagrams

All samples are available in the evaluation_results.json file

In this example we can see 2 generated diagrams that are of similar quality. It's difficult to say which one is more helpful to the user.
Note: It's not a stretch to think that in this case the user would prefer to "visualize" its code in a different way than a diagram
```
    {
        "sample_index": 52,
        "question": "private List<double[][][]> gradientsHistory = new List<double[][][]>(); Add filling gradientsHistory",
        "answer": "To fill the `gradientsHistory` list with the gradients calculated during backpropagation, you can modify your existing `BackPropagationSGDM` method to store the gradients at each layer. Here's how you can update the method to add the gradients to the `gradientsHistory` list:\n\n```csharp\nprivate void BackPropagationSGDM(double[] predicted, double[] targets)\n{\n    // Your existing code...\n\n    List<double[][]> layerGradients = new List<double[][]>();\n\n    for (int layerIndex = onlineLayers.Length - 2; layerIndex >= 0; layerIndex--)\n    {\n        // Your existing code for backpropagation...\n\n        double[][] layerGradient = new double[currentLayer.size][];\n        for (int i = 0; i < currentLayer.size; i++)\n        {\n            layerGradient[i] = new double[nextLayer.size];\n            for (int j = 0; j < nextLayer.size; j++)\n            {\n                // Calculate the gradient for the weights\n                layerGradient[i][j] = errors[j] * DSwishActivation(nextLayer.neurons[j]) * currentLayer.neurons[i];\n            }\n        }\n\n        layerGradients.Add(layerGradient);\n\n        // Update weights and biases...\n\n        // Update errors for the next layer...\n    }\n\n    gradientsHistory.Add(layerGradients);\n}\n```\n\nIn this updated method:\n- A new list `layerGradients` is created to store the gradients for each layer.\n- Inside the loop for each layer, a 2D array `layerGradient` is created to store the gradients for that layer's weights.\n- The gradients for the weights are calculated and stored in `layerGradient`.\n- `layerGradient` is added to `layerGradients` for the current layer.\n- Finally, `layerGradients` is added to the `gradientsHistory` list.\n\nThis way, `gradientsHistory` will store the gradients for each layer during the backpropagation process. Feel free to adjust the code as needed based on your specific requirements or data structure.",
        "gpt-4-turbo": {
            "name": "gpt-4-0125-preview",
            "diagram": "```mermaid\ngraph TD\n    A[Start BackPropagationSGDM] --> B[Initialize layerGradients]\n    B --> C{For each layer}\n    C -->|For last to first layer| D[Calculate layerGradient]\n    D --> E[Store gradients in layerGradient]\n    E --> F[Add layerGradient to layerGradients]\n    F --> G{More layers?}\n    G -->|Yes| C\n    G -->|No| H[Add layerGradients to gradientsHistory]\n    H --> I[End BackPropagationSGDM]\n```",
            "valid_syntax": true,
            "response_time": 5.5360400676727295
        },
        "fine-tuned-gpt-3.5": {
            "name": "ft:gpt-3.5-turbo-0613:rubrick-ai::8wWjQ3wc",
            "diagram": "```mermaid\ngraph LR\nA[BackPropagationSGDM] --> B[Create layerGradients list]\nB --> C[Loop through layers]\nC --> D[Create layerGradient for current layer]\nD --> E[Calculate gradients for weights]\nE --> F[Add layerGradient to layerGradients]\nF --> C\nC --> G[Add layerGradients to gradientsHistory]\nG --> H[End]\n```",
            "valid_syntax": true,
            "response_time": 1.5658812522888184
        }
    },
```

In this sample our model generated a simpler diagram than gpt-4-turbo but it's hard to say if that's of higher and lower quality. We could argue that neither are very useful
```
    {
        "sample_index": 68,
        "question": "why first version of tree with depth didn't update grandchildren?",
        "answer": "The first version of the `moveNode` method did not update the grandchildren because it only updated the depth of the moved node and its direct children. It did not recursively update the depth of all descendants (grandchildren, great-grandchildren, etc.) of the moved node.\n\nIn the first version, the `moveNode` method only updated the depth of the moved node based on its new parent's depth. However, it did not traverse the entire subtree rooted at the moved node to update the depth of all descendants. As a result, only the immediate children of the moved node had their depths updated, while the grandchildren and subsequent descendants were left with incorrect depth values.\n\nTo ensure that all descendants have their depths updated correctly after moving a node, it is necessary to recursively update the depth of all children, grandchildren, and subsequent descendants of the moved node. This recursive update ensures that the entire subtree rooted at the moved node maintains the correct depth values relative to the new parent node.",
        "gpt-4-turbo": {
            "name": "gpt-4-0125-preview",
            "diagram": "```mermaid\ngraph TD\n    A[moveNode Method] --> B[Update Depth of Moved Node]\n    A --> C[Update Depth of Direct Children]\n    A --> D[Missed: Update Depth of Grandchildren and Beyond]\n    D --> E[Required: Recursive Update of All Descendants]\n    E --> F[Correct Depth Values for Entire Subtree]\n```",
            "valid_syntax": true,
            "response_time": 6.81128716468811
        },
        "fine-tuned-gpt-3.5": {
            "name": "ft:gpt-3.5-turbo-0613:rubrick-ai::8wWjQ3wc",
            "diagram": "```mermaid\ngraph LR\nA[Move Node] --> B[Update Depth of Moved Node]\nB --> C[Update Depth of Direct Children]\nC --> D[Update Depth of Grandchildren]\nD --> E[Update Depth of Subsequent Descendants]\n```",
            "valid_syntax": true,
            "response_time": 1.1204769611358643
        }
    }
```

In this sample we can also see 2 diagrams that are of similar quality
```
    {
        "sample_index": 70,
        "question": "create script in C# that converts string encoded in 8859-5 to UTF\n\n",
        "answer": "Sure! Here's a C# script that converts a string encoded in ISO-8859-5 to UTF-8:\n\n```csharp\nusing System;\nusing System.Text;\n\npublic class Converter\n{\n    public static string ConvertToUtf8(string input)\n    {\n        // Create ISO-8859-5 encoding\n        Encoding iso8859_5 = Encoding.GetEncoding(\"iso-8859-5\");\n\n        // Convert the input string to bytes using ISO-8859-5 encoding\n        byte[] isoBytes = iso8859_5.GetBytes(input);\n\n        // Convert the ISO-8859-5 bytes to UTF-8 bytes\n        byte[] utf8Bytes = Encoding.Convert(iso8859_5, Encoding.UTF8, isoBytes);\n\n        // Convert the UTF-8 bytes back to a string\n        string utf8String = Encoding.UTF8.GetString(utf8Bytes);\n\n        return utf8String;\n    }\n}\n\npublic class Program\n{\n    public static void Main(string[] args)\n    {\n        string input = \"Your ISO-8859-5 encoded string here\";\n\n        string utf8String = Converter.ConvertToUtf8(input);\n\n        Console.WriteLine(\"UTF-8 string: \" + utf8String);\n    }\n}\n```\n\nReplace `\"Your ISO-8859-5 encoded string here\"` with the actual ISO-8859-5 encoded string you want to convert. When you run the script, it will output the converted UTF-8 string.\n\nPlease note that this script assumes that the input string is encoded in ISO-8859-5. If the actual encoding is different, you may need to modify the script accordingly.",
        "gpt-4-turbo": {
            "name": "gpt-4-0125-preview",
            "diagram": "```mermaid\ngraph TD\n    A[Start] --> B[Create ISO-8859-5 Encoding]\n    B --> C[Convert Input String to Bytes using ISO-8859-5]\n    C --> D[Convert ISO-8859-5 Bytes to UTF-8 Bytes]\n    D --> E[Convert UTF-8 Bytes back to String]\n    E --> F[End]\n```",
            "valid_syntax": true,
            "response_time": 3.578355073928833
        },
        "fine-tuned-gpt-3.5": {
            "name": "ft:gpt-3.5-turbo-0613:rubrick-ai::8wWjQ3wc",
            "diagram": "```mermaid\ngraph LR\nA[Input String] --> B(ConvertToUtf8)\nB --> C[ISO-8859-5 Encoding]\nC --> D[Convert to Bytes]\nD --> E[UTF-8 Encoding]\nE --> F[Convert to Bytes]\nF --> G[UTF-8 String]\nG --> H[Output]\n```",
            "valid_syntax": true,
            "response_time": 1.3756351470947266
        }
    },
```

This is a example of where it can be useful to visualize the answer using a diagram. Both models produced a good diagram. The one produced by gpt-4-turbo has a better looks and seems to be a better representation of what the process could look like, but our model produced a diagram that seems to be a better representation of what was the requirements of the user and the answer that was provided.
I prefer the one that gpt-4 produced but it also seems that gpt-4 went a bit further to produce a diagram with info/detail that was not in the answer, like it added more thought to the already provided answer.
```
    {
        "sample_index": 83,
        "question": "Acceptance criteria:\n\nAs an api consumer, given username and header \u201cAccept: application/json\u201d, I would like to list all his github repositories, which are not forks. Information, which I require in the response, is:\n\n \n\nRepository Name\n\nOwner Login\n\nFor each branch it\u2019s name and last commit sha\n\n \n\nAs an api consumer, given not existing github user, I would like to receive 404 response in such a format:\n\n{\n\n    \u201cstatus\u201d: ${responseCode}\n\n    \u201cmessage\u201d: ${whyHasItHappened}\n\n}\n\n \n\nNotes:\n\nPlease full-fill the given acceptance criteria, delivering us your best code compliant with industry standards.\n\nPlease use https://developer.github.com/v3 as a backing API\n\nApplication should have a proper README.md file",
        "answer": "Adrenaline, a programming assistant\n\nTo list all the GitHub repositories of a user (excluding forks) and retrieve the required information, you can use the GitHub REST API. Here's an example of how you can achieve this using the cURL command:\n\n```bash\ncurl -H \"Accept: application/vnd.github.v3+json\" https://api.github.com/users/{username}/repos?per_page=100\n```\n\nReplace `{username}` with the actual GitHub username you want to retrieve the repositories for.\n\nThis command will return a JSON response containing an array of repositories. For each repository, you can extract the following information:\n\n- Repository Name: Access the `\"name\"` field of each repository object.\n- Owner Login: Access the `\"owner.login\"` field of each repository object.\n- Branch Name and Last Commit SHA: For each repository, you can make another API call to retrieve the branches and their last commit SHA using the following command:\n\n```bash\ncurl -H \"Accept: application/vnd.github.v3+json\" https://api.github.com/repos/{owner}/{repo}/branches\n```\n\nReplace `{owner}` with the repository owner's GitHub username and `{repo}` with the repository name.\n\nThis command will return a JSON response containing an array of branches. For each branch, you can extract the following information:\n\n- Branch Name: Access the `\"name\"` field of each branch object.\n- Last Commit SHA: Access the `\"commit.sha\"` field of each branch object.\n\nYou can write a script in your preferred programming language to automate this process and format the response as required. Remember to handle error cases, such as the user not existing, by checking the response code and providing an appropriate response message.\n\nRegarding the README.md file, please create a file named `README.md` in the root directory of your application and provide relevant information about your application, including installation instructions, usage examples, and any other necessary details.\n\nI hope this helps! Let me know if you have any further questions.",
        "gpt-4-turbo": {
            "name": "gpt-4-0125-preview",
            "diagram": "```mermaid\ngraph TD\n    A[Start] --> B{Check if GitHub user exists}\n    B -->|Yes| C[Retrieve user repositories excluding forks]\n    B -->|No| D[Return 404 response]\n    C --> E[For each repository]\n    E -->|API call| F[Retrieve branches and last commit SHA]\n    F --> G{Is there another repository?}\n    G -->|Yes| E\n    G -->|No| H[Format response]\n    H --> I[End]\n    D --> I\n```",
            "valid_syntax": true,
            "response_time": 7.892364025115967
        },
        "fine-tuned-gpt-3.5": {
            "name": "ft:gpt-3.5-turbo-0613:rubrick-ai::8wWjQ3wc",
            "diagram": "```mermaid\ngraph LR\nA[API Consumer] --> B(cURL Command)\nB --> C{Retrieve Repositories}\nC --> D[Extract Repository Name]\nC --> E[Extract Owner Login]\nC --> F[Make API Call for Branches]\nF --> G{Retrieve Branches}\nG --> H[Extract Branch Name]\nG --> I[Extract Last Commit SHA]\nH --> J\nI --> J\nC --> J\nJ --> K{Handle Error Cases}\nK --> L[Return Response]\n```",
            "valid_syntax": true,
            "response_time": 2.0781891345977783
        }
    },
```

The examples analyzed are not enough to take solid conclusions and more work is needed for a proper assessment. At this time it seems fair to say that the quality of the output is not greatly different between models.

More related to Quality of the output seems to be the type of question and answer. Questions that relate with architectural topics seems to be a better fit for diagram. Small changes in code would probably benefit more from different types of visualizations. 

# Conclusions

GPT is in-fact poor at generating mermaidJS diagram with proper syntax. This is mainly because the syntax is error prone and the documentation is extensive. 

We succeed in creating a model that has the same accuracy as gpt-4-turbo that is much cheaper and faster, having spent only ~$20 in dataset generation on fine-tuning costs.

# Next steps

This was what was possible to achieve in a few days challenge but there are many "low hanging fruits" that we can try to improve the results. Some are:

- Generate a bigger dataset - training with less than 50 samples is not ideal
- Generate the dataset with the CLI validator instead of the npm package, which is not working properly
- Fine-tune the model for longer than 100 steps
- Better curate the mermaidJS documentation and write it in a format better suited to LLMs, with examples for corner cases where the model usually struggles
- Break-down the process in multiple calls to the model when generating each diagram

Another note, and reaching outside of the scope of this project, is that it could be worth to explore different type of visualization depending on the topic being asked. The Quality of the output seem to be more influenced by that than the quality of the models.